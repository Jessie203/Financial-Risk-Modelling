{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01f5ba32-1236-44df-94b9-0c796200ab19",
   "metadata": {},
   "source": [
    "# Phase 2: Feature Engineering\n",
    "\n",
    "Having completed our EDA, we now clean the data, engineer predictive features, and prepare everything for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deaa4a28-ee56-44b9-ab8c-dcfe64e8e776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load cleaned subset from Phase 1 (or re‐load raw and apply initial filters)\n",
    "df = pd.read_csv('/Users/Cathaml/Desktop/accepted_cleaned.csv', header=0, low_memory=False)\n",
    "df = df[df.loan_status.isin(['Fully Paid','Charged Off'])].copy()\n",
    "df['default_flag'] = (df.loan_status == 'Charged Off').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667773f3-98cd-43c8-a76d-8ff4d573b5ce",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning & Preprocessing\n",
    "\n",
    "We first convert types, drop unneeded columns, and impute or encode missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d82c1ce-b0f7-469e-9ba8-93e55c1d264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Drop columns with >80% missing\n",
    "missing = df.isnull().mean()\n",
    "to_drop = missing[missing > 0.8].index\n",
    "df.drop(columns=to_drop, inplace=True)\n",
    "\n",
    "# 1.2 Robustly strip “%” and convert to float\n",
    "for col in ['int_rate', 'revol_util']:\n",
    "    # cast to string, remove “%”, then coerce to numeric\n",
    "    df[col] = pd.to_numeric(\n",
    "        df[col]\n",
    "          .astype(str)\n",
    "          .str.replace('%', '', regex=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "# 1.3 Parse dates\n",
    "df['issue_d'] = pd.to_datetime(df['issue_d'], format='%b-%Y')\n",
    "df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'], format='%b-%Y')\n",
    "\n",
    "# 1.4 Impute numeric and encode categoricals\n",
    "num_cols = df.select_dtypes(include='number').columns.drop('default_flag')\n",
    "df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "\n",
    "cat_cols = ['term', 'grade', 'sub_grade', 'home_ownership', 'verification_status']\n",
    "for c in cat_cols:\n",
    "    df[c] = df[c].fillna('Missing').astype('category')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8cb591-d7a7-4b5b-9729-c63f8326912f",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "\n",
    "Based on our EDA insights, we create:\n",
    "- **Binned loan amounts** at LendingClub’s tiers  \n",
    "- **Debt-to-income buckets**  \n",
    "- **Credit-utilization ratios**  \n",
    "- **Vintage features** from issue date  \n",
    "- **Ordinal encodings** for grade/subgrade  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a01d813a-6726-4ccb-93d4-ebc66c505b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Loan‐amount bins\n",
    "tiers = [0,5000,10000,15000,20000,25000,30000,35000,40000]\n",
    "labels = [f\"{tiers[i]}–{tiers[i+1]}\" for i in range(len(tiers)-1)]\n",
    "df['amt_bin'] = pd.cut(df['loan_amnt'], bins=tiers, labels=labels)\n",
    "\n",
    "# 2.2 DTI bucket\n",
    "df['dti_bin'] = pd.qcut(df['dti'], 5, labels=False)\n",
    "\n",
    "# 2.3 Credit utilization proxy\n",
    "df['util_ratio'] = df['revol_bal'] / (df['funded_amnt'] + 1)\n",
    "\n",
    "# 2.4 Vintage features\n",
    "df['issue_year']  = df['issue_d'].dt.year\n",
    "df['issue_month'] = df['issue_d'].dt.month\n",
    "\n",
    "# 2.5 Ordinal encode grade/subgrade\n",
    "grade_map = {g: i for i, g in enumerate(sorted(df['grade'].cat.categories))}\n",
    "df['grade_ord'] = df['grade'].map(grade_map)\n",
    "df['subgrade_ord'] = df['sub_grade'].cat.codes  # preserves A1–G5 order\n",
    "\n",
    "# 2.6 One‐hot encode rare categoricals\n",
    "df = pd.get_dummies(df, columns=['term','home_ownership','verification_status','amt_bin'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdea80c-1910-420e-9088-f0889e0ea155",
   "metadata": {},
   "source": [
    "We save the processed dataset for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23acaa59-e4cd-420b-9c50-612e9c04fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/Users/Cathaml/Desktop/loans_fe.csv.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
